df.pprogram: word_river/run_train.py
method: bayes
metric:
  goal: maximize
  name: best_f1_score
parameters:
  sents_win:
    values:
      - 20
      - 35
      - 40
      - 50
      - 60
      - 70
  step_size:
    value: 20
  save_checkpoints:
    value: 0
  max_source_length:
    values:
      - 100
      - 120
  per_device_train_batch_size:
    value: 1
  accumulate_grad_batches:
    distribution: int_uniform
    min: 1
    max: 16
  n_data_points:
    value: 500
  ds_dir:
    value: "18_12_20"
  model_name_or_path:
    distribution: categorical
    values:
      - roberta-base
      - roberta-large
      - bert-base-uncased
      - bert-large-uncased
      - bert-large-uncased-whole-word-masking
      - transfo-xl-wt103
      - xlnet-base-cased
      - xlnet-large-cased
      - roberta-large-mnli
      - distilbert-base-uncased
      - albert-base-v2
      - albert-large-v2
      - albert-xlarge-v2
      - albert-xxlarge-v2
      - allenai/longformer-base-4096
      - allenai/longformer-large-4096
      - microsoft/deberta-base
      - microsoft/deberta-large
  loss:
    distribution: categorical
    values:
      - cross_entropy
      - focal
      - f1
  finetune:
    distribution: categorical
    values:
      - 0
      - 1
  lr_1:
    distribution: log_uniform
    min: -14
    max: -2
  lr_2:
    distribution: log_uniform
    min: -12
    max: -1
  lr_3:
    distribution: log_uniform
    min: -12
    max: -1
  project:
    value: private_example
  entity:
    value: roman-chugunov
