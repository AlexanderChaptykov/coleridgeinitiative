{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMpKKe76M1kD"
      },
      "source": [
        "import os\n",
        "import platform\n",
        "import sys\n",
        "\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
        "if platform.system() == 'Linux':\n",
        "    % load_ext autoreload\n",
        "    % autoreload 2\n",
        "\n",
        "\n",
        "    from sys import platform\n",
        "    from google.colab import drive\n",
        "    import shutil\n",
        "\n",
        "\n",
        "    def mount_drive():\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "        # main_folder_path = \"drive/My Drive/nlp/\"\n",
        "        # sys.path.append(os.path.abspath(main_folder_path))\n",
        "        # os.environ['PYTHONPATH'] += \":/content/drive/MyDrive/nlp/\"\n",
        "\n",
        "\n",
        "    # noinspection PyUnresolvedReferences\n",
        "    def install_deps():\n",
        "        ! pip install python-Levenshtein -q\n",
        "        ! pip install pytorch_lightning==1.1.4 -q\n",
        "        ! pip install wandb -q\n",
        "        ! pip install transformers -q\n",
        "        ! pip install pydantic -q\n",
        "        ! pip install --upgrade --force-reinstall --no-deps kaggle -q\n",
        "        ! pip install -q kaggle\n",
        "        ! pip install -q datasets\n",
        "        ! pip install clean-text\n",
        "        ! pip install lightning-bolts\n",
        "        # !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n",
        "\n",
        "\n",
        "    def download_kaggle():\n",
        "        if not os.path.exists(\"~/.kaggle/\") or 'kaggle.json' not in os.listdir():\n",
        "            os.system('mkdir ~/.kaggle')\n",
        "            shutil.copyfile('drive/MyDrive/data/kaggle.json', \"kaggle.json\")\n",
        "            os.system('cp kaggle.json ~/.kaggle/')\n",
        "            os.system('chmod 600 ~/.kaggle/kaggle.json')\n",
        "            os.system('kaggle competitions download -c coleridgeinitiative-show-us-the-data')\n",
        "            os.system('mkdir input')\n",
        "            os.system('unzip -q coleridgeinitiative-show-us-the-data.zip -d input/coleridgeinitiative-show-us-the-data')\n",
        "\n",
        "\n",
        "    if not os.path.exists(\"/content/drive\"):\n",
        "        mount_drive()\n",
        "        install_deps()\n",
        "        download_kaggle()\n",
        "    sys.path.append(os.path.abspath('/content/drive/MyDrive/nlp-master'))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwheUHNNMzQo"
      },
      "source": [
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "CV_F = 0\n",
        "import os\n",
        "sys.argv = ['-f ']\n",
        "os.environ[\"WANDB_API_KEY\"] = 'f7a90002357566fdfb99c6768a70a2ec502afe35'\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from word_river.model.dataset.utils import cleaning, get_augs, get_weights\n",
        "from word_river.model.pl.utils import PrintLogger\n",
        "from word_river.train_data.utils import get_publications, Ranger\n",
        "from word_river.cli_parser.train import model_args, training_args, wandb_args, data_args\n",
        "from word_river.train_data.prepare_cval import Spliter\n",
        "from word_river.model.dataset import Collator\n",
        "from word_river.dtypes import Item, Publication\n",
        "\n",
        "from pathlib import Path\n",
        "import multiprocessing as mp\n",
        "from datasets import load_dataset\n",
        "import random, nltk\n",
        "from word_river.model.architecture import MyModel\n",
        "from word_river.model.pl import ModelPl\n",
        "\n",
        "nltk.download('punkt')\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "model_args.model_name_or_path = 'distilbert-base-cased'\n",
        "wandb_args.project, wandb_args.entity = 'coleridgeinitiative', 'alexch'\n",
        "\n",
        "data_args.max_source_length = 120\n",
        "data_args.ds_dir = Path('/Users/alexch/tmp/input/coleridgeinitiative-show-us-the-data')\n",
        "data_args.num_workers = mp.cpu_count()\n",
        "data_args.GET_EVERY_PUB = 4\n",
        "\n",
        "training_args.output_dir = Path('/Users/alexch/tmp/input/checkpoint')\n",
        "training_args.early_stopping_patience = 5\n",
        "training_args.mixed_precision = True\n",
        "training_args.gpus = '0'\n",
        "training_args.lr_1 = 3e-5\n",
        "training_args.lr_2 = .01\n",
        "training_args.per_device_train_batch_size = 40\n",
        "training_args.early_stopping_patience = 33\n",
        "training_args.augs_csv = '/Users/alexch/tmp/input/data_set_800.csv'\n",
        "\n",
        "len_ones = 1000\n",
        "len_zer = 9000\n",
        "\n",
        "re_write = False\n",
        "item_data = Path('/Users/alexch/tmp/input/item_folder')\n",
        "path_train_items = item_data / 'train_items_2.pickle'\n",
        "path_test_items = item_data / 'test_items_2.pickle'\n",
        "path_pubs = item_data / 'pubs.csv'\n",
        "training_args.gpus = None\n",
        "\n",
        "if platform.system() == 'Linux':\n",
        "    item_data = Path('/content/drive/MyDrive/data/')\n",
        "    path_train_items = item_data / 'train_items_2.pickle'\n",
        "    path_test_items = item_data / 'test_items_2.pickle'\n",
        "    training_args.augs_csv = '/content/drive/MyDrive/data/govt2/data_set_800.csv'\n",
        "    training_args.output_dir = Path('drive/MyDrive/checkpoint')\n",
        "    data_args.ds_dir = Path('input/coleridgeinitiative-show-us-the-data')\n",
        "    training_args.gpus = '0'\n",
        "    path_all_items = item_data / 'all_items.csv'\n",
        "\n",
        "\n",
        "def get_train_test_items():\n",
        "    if path_all_items.exists():\n",
        "        print('Loading items...')\n",
        "        all_items: pd.DataFrame = pd.read_csv(path_all_items, compression='zip')\n",
        "        all_items['char_range'] = [eval(x) for x in all_items['char_range']]\n",
        "\n",
        "        def nans(v):\n",
        "            if type(v) == str:\n",
        "                return v\n",
        "            if np.isnan(v):\n",
        "                return\n",
        "            assert False\n",
        "\n",
        "        all_items['dataset_title'] = [nans(x) for x in all_items['dataset_title']]\n",
        "        all_items['dataset_label'] = [nans(x) for x in all_items['dataset_label']]\n",
        "\n",
        "    else:\n",
        "        print('Creating items...')\n",
        "\n",
        "        if path_pubs.exists():\n",
        "            pubs = pd.read_csv(path_pubs)\n",
        "            pubs.dataset_title = [eval(x) for x in pubs.dataset_title]\n",
        "            pubs.dataset_label = [eval(x) for x in pubs.dataset_label]\n",
        "        else:\n",
        "            df = pd.read_csv(data_args.ds_dir / 'train.csv')\n",
        "            pubs: pd.DataFrame = get_publications(df)\n",
        "            with mp.Pool(mp.cpu_count()) as p:\n",
        "                pubs[\"text\"] = list(tqdm(p.imap(cleaning, pubs['text']), total=len(pubs['text']), desc='Split'))\n",
        "            # noinspection PyUnresolvedReferences\n",
        "            pubs.to_csv(path_pubs, index=False)\n",
        "\n",
        "        pubs = [Publication(**rec) for rec in pubs.to_dict('records')]\n",
        "\n",
        "        spliter = Spliter(data_args, 'sent_tokenize', ranger=Ranger(data_args, mode='get_longest'), get_first=False)\n",
        "\n",
        "        all_items: List[Item] = spliter(pubs)\n",
        "        all_items = pd.DataFrame([i.dict() for i in all_items])\n",
        "        # noinspection PyUnresolvedReferences\n",
        "        all_items['pub_title'] = [x.strip() for x in all_items['pub_title']]\n",
        "        # noinspection PyUnresolvedReferences\n",
        "        all_items.to_csv(path_all_items, compression=\"zip\", index=None)\n",
        "\n",
        "    # noinspection PyUnresolvedReferences\n",
        "    all_items = [Item(**rec) for rec in all_items.to_dict('records')]\n",
        "    print('len(all_items)', len(all_items))\n",
        "\n",
        "    all_items = [x for x in all_items if 'data' in x.text.lower() or 'stud' in x.text.lower()]\n",
        "\n",
        "    adni_filter = lambda item: item.dataset_label == 'adni' and not (\n",
        "            'sample' in item.text or 'data' in item.text or 'stud' in item.text)\n",
        "    all_items = [x for x in all_items if not adni_filter(x)]\n",
        "\n",
        "    # reduce  ADNI\n",
        "    all_items = \\\n",
        "        random.sample(\n",
        "            [x for x in all_items if x.dataset_title == \"Alzheimer's Disease Neuroimaging Initiative (ADNI)\"],\n",
        "            2500) + \\\n",
        "        [x for x in all_items if x.dataset_title != \"Alzheimer's Disease Neuroimaging Initiative (ADNI)\"]\n",
        "\n",
        "    conll2003 = load_dataset('conll2003')\n",
        "    neg_texts = [' '.join(x) for x in conll2003['train']['tokens']]\n",
        "    neg_items = []\n",
        "    for txt in neg_texts:\n",
        "        neg_items.append(\n",
        "            Item(\n",
        "                pub_title='conll2003',\n",
        "                dataset_title=None,\n",
        "                dataset_label=None,\n",
        "                text=txt,\n",
        "                char_range=(1, 1)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    title_stat = pd.Series([x.dataset_title for x in all_items if x.dataset_title]).value_counts()\n",
        "\n",
        "    # 1 choose test titles\n",
        "    test_titles = set(title_stat[CV_F::5].index)\n",
        "    train_titles = set([x for x in title_stat.index if x not in test_titles])\n",
        "\n",
        "    # 2 select train/test publications\n",
        "    test_pubs = set([x.pub_title for x in all_items if x.dataset_title in test_titles])\n",
        "    train_pubs = set([x.pub_title for x in all_items]) - test_pubs\n",
        "\n",
        "    # 3 create train/test items\n",
        "    train_items = [x for x in all_items if x.pub_title in train_pubs and x.dataset_title not in test_titles]\n",
        "    test_items = [x for x in all_items if x.pub_title in test_pubs and x.dataset_title not in train_titles]\n",
        "\n",
        "    train_items = [x for x in train_items if x.dataset_title] + neg_items + random.sample(\n",
        "        [x for x in train_items if not x.dataset_title], 15000)\n",
        "    return train_items, test_items\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvGMbb2WNk8b"
      },
      "source": [
        "train_items, test_items = get_train_test_items()\n",
        "\n",
        "# noinspection PyTypeChecker\n",
        "dl_test = DataLoader(random.sample([x for x in test_items if x.dataset_title], len_ones) + \\\n",
        "                     random.sample([x for x in test_items if not x.dataset_title], len_zer),\n",
        "                     collate_fn=Collator(data_args, model_args),\n",
        "                     batch_size=120,\n",
        "                     num_workers=data_args.num_workers,\n",
        "                     shuffle=False\n",
        "                     )\n",
        "\n",
        "dl = DataLoader(train_items,\n",
        "                collate_fn=Collator(data_args,\n",
        "                                    model_args,\n",
        "                                    augmentation_prob=0.4,\n",
        "                                    augmentation_list=get_augs(training_args, data_args),\n",
        "                                    ),\n",
        "                # sampler=RandomSampler(train_items, num_samples=400, replacement=True),\n",
        "                sampler=WeightedRandomSampler(get_weights(train_items), 900, replacement=True),\n",
        "                batch_size=training_args.per_device_train_batch_size,\n",
        "                # batch_size=training_args.per_device_train_batch_size,\n",
        "                num_workers=data_args.num_workers,\n",
        "                shuffle=False\n",
        "                )\n",
        "\n",
        "model = ModelPl(MyModel, model_args, data_args, training_args, None)\n",
        "training_args.callbacks_monitor = 'fbeta_val'\n",
        "\n",
        "logers = [\n",
        "    WandbLogger(config={**model_args.__dict__, **data_args.__dict__, **training_args.__dict__}),\n",
        "    PrintLogger()\n",
        "]\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        dirpath=training_args.output_dir,\n",
        "        filename='model',\n",
        "        monitor=training_args.callbacks_monitor,\n",
        "        mode=training_args.callbacks_mode,\n",
        "        verbose=True),\n",
        "\n",
        "    EarlyStopping(\n",
        "        monitor=training_args.callbacks_monitor, min_delta=0.001,\n",
        "        patience=training_args.early_stopping_patience,\n",
        "        verbose=True,\n",
        "        mode=training_args.callbacks_mode),\n",
        "    LearningRateMonitor(logging_interval=\"epoch\"),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    precision=(training_args.mixed_precision and 16) or 32,\n",
        "    gpus=training_args.gpus,\n",
        "    #   logger=wandb_logger,\n",
        "    #  max_epochs=33,\n",
        "    logger=logers,\n",
        "    #   logger=PrintLogger(),\n",
        "    num_sanity_val_steps=0,\n",
        "    # callbacks=callbacks\n",
        "    # tpu_cores=7\n",
        ")\n",
        "\n",
        "trainer.tune(model, dl, dl_test)\n",
        "trainer.fit(model, dl, dl_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}